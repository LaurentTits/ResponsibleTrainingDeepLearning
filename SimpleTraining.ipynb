{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjqV1qgoit8K6WWKPGY/EQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaurentTits/ResponsibleTrainingDeepLearning/blob/main/SimpleTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vérification de la sélection du GPU**"
      ],
      "metadata": {
        "id": "dFqaE8z-uNGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "1e92dwCMuRNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Installation de pytorch lightning**"
      ],
      "metadata": {
        "id": "w8_iTnPpuTC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning torchsummary pytorch_bench captum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6g-KwsCuTaZ",
        "outputId": "f2897d8a-d4ec-4e67-ecb9-f2bd885ad663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Collecting pytorch_bench\n",
            "  Downloading pytorch_bench-0.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting captum\n",
            "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.10.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.6.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from pytorch_bench) (12.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pytorch_bench) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch_bench) (1.26.4)\n",
            "Collecting torchprofile (from pytorch_bench)\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Collecting codecarbon (from pytorch_bench)\n",
            "  Downloading codecarbon-2.8.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Collecting arrow (from codecarbon->pytorch_bench)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon->pytorch_bench) (8.1.8)\n",
            "Collecting fief-client[cli] (from codecarbon->pytorch_bench)\n",
            "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from codecarbon->pytorch_bench) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon->pytorch_bench) (0.21.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon->pytorch_bench) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon->pytorch_bench) (9.0.0)\n",
            "Collecting questionary (from codecarbon->pytorch_bench)\n",
            "  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting rapidfuzz (from codecarbon->pytorch_bench)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from codecarbon->pytorch_bench) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from codecarbon->pytorch_bench) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from codecarbon->pytorch_bench) (0.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch_bench) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch_bench) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch_bench) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch_bench) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch_bench) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch_bench) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pytorch_bench) (2.8.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->pytorch_bench) (12.570.86)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.11/dist-packages (from torchprofile->pytorch_bench) (0.20.1+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pytorch_bench) (1.17.0)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon->pytorch_bench)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon->pytorch_bench)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon->pytorch_bench)\n",
            "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting yaspin (from fief-client[cli]->codecarbon->pytorch_bench)\n",
            "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon->pytorch_bench) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon->pytorch_bench) (2025.1)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from questionary->codecarbon->pytorch_bench) (3.0.50)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon->pytorch_bench) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon->pytorch_bench) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon->pytorch_bench) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon->pytorch_bench) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon->pytorch_bench) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon->pytorch_bench) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon->pytorch_bench) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pytorch_bench) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pytorch_bench) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pytorch_bench) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon->pytorch_bench) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.11/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pytorch_bench) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon->pytorch_bench) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon->pytorch_bench) (0.2.13)\n",
            "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon->pytorch_bench)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pytorch_bench) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon->pytorch_bench) (2.22)\n",
            "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_bench-0.1.2-py3-none-any.whl (6.6 kB)\n",
            "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.0-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.2-py3-none-any.whl (931 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading codecarbon-2.8.3-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.7/516.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-2.1.0-py3-none-any.whl (36 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
            "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
            "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: types-python-dateutil, termcolor, rapidfuzz, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, yaspin, questionary, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpx, arrow, nvidia-cusolver-cu12, jwcrypto, fief-client, torchmetrics, captum, torchprofile, pytorch_lightning, codecarbon, pytorch_bench\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.5.0\n",
            "    Uninstalling termcolor-2.5.0:\n",
            "      Successfully uninstalled termcolor-2.5.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Chargement des librairies**"
      ],
      "metadata": {
        "id": "rRkFBLBesp8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision import transforms\n",
        "from torchmetrics.classification import MulticlassConfusionMatrix, Accuracy\n",
        "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from pytorch_bench import benchmark\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import random\n",
        "import wandb\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "kNjgycFt9qB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Google drive**"
      ],
      "metadata": {
        "id": "gThTjOWI0NNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset paths\n",
        "dataset_folder = '/content/drive/My Drive/ResponsibleTraining/Datasets'\n",
        "images_tar_path = os.path.join(dataset_folder, 'ILSVRC2012_img_val.tar')\n",
        "bbox_tgz_path = os.path.join(dataset_folder, 'ILSVRC2012_bbox_val_v3.tgz')\n",
        "extract_path = '/content/ILSVRC2012_val'\n",
        "bbox_extract_path = '/content/ILSVRC2012_bbox'\n",
        "bbox_annotation_path = f'{bbox_extract_path}/val'\n",
        "\n",
        "def extract_tar_files(file_tar_path, mode, extract_path):\n",
        "  \"\"\"Extracts tar/tgz files if not already extracted.\"\"\"\n",
        "  if not os.path.exists(extract_path):\n",
        "      os.makedirs(extract_path, exist_ok=True)\n",
        "      with tarfile.open(file_tar_path, mode) as tar:\n",
        "          tar.extractall(path=extract_path)\n",
        "\n",
        "  # Count extracted elements\n",
        "  num_files = len(os.listdir(extract_path))\n",
        "  print(f\"Total extracted elements (file or folder) in the extract path: {num_files}\")\n",
        "\n",
        "# Extract images if not already extracted\n",
        "extract_tar_files(images_tar_path, 'r', extract_path)\n",
        "# Get sorted list of image files\n",
        "image_files = sorted([f for f in os.listdir(extract_path) if f.endswith('.JPEG')])\n",
        "\n",
        "# Extract bounding boxes if not already extracted\n",
        "extract_tar_files(bbox_tgz_path, 'r:gz', bbox_extract_path)\n",
        "\n",
        "# Load json with class labels for imagenet\n",
        "with open(os.path.join(dataset_folder, 'imagenet_class_index.json'), 'r') as f:\n",
        "    class_mapping = json.load(f)\n",
        "\n",
        "# Define the directory to save the best model\n",
        "models_folder = '/content/drive/My Drive/ResponsibleTraining/Models'\n",
        "os.makedirs(models_folder, exist_ok=True)\n",
        "\n",
        "# Convert WordNet ID to readable class name\n",
        "def get_class_name(wnid):\n",
        "    for key, value in class_mapping.items():\n",
        "        if value[0] == wnid:\n",
        "            return value[1].replace('_', ' ')\n",
        "    return 'Unknown'"
      ],
      "metadata": {
        "id": "8ZY6d-P20OCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connexion à wandb"
      ],
      "metadata": {
        "id": "mtoXPvfFBTV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(dataset_folder, 'wandb_key.txt'), 'r') as f:\n",
        "    wandb_key = f.read().strip()\n",
        "\n",
        "!pip install wandb\n",
        "!wandb login {wandb_key}\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"ResponsibleTraining\", config={\n",
        "    \"learning_rate\": Learning_rate,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"model\": \"mobilenet_v2\"\n",
        "})"
      ],
      "metadata": {
        "id": "CJDdT6EzBT05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first 9 images with bounding boxes (3x3 grid)\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(9):\n",
        "    image_path = os.path.join(extract_path, image_files[i])\n",
        "    image_name = os.path.splitext(image_files[i])[0]  # Remove .JPEG\n",
        "    annotation_path = os.path.join(bbox_annotation_path, f'{image_name}.xml')\n",
        "\n",
        "    if not os.path.exists(annotation_path):\n",
        "        print(f'Annotation not found for {image_files[i]}')\n",
        "        continue\n",
        "\n",
        "    # Load image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Parse XML annotation\n",
        "    tree = ET.parse(annotation_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Get class name\n",
        "    wnid = root.find('object/name').text if root.find('object/name') is not None else 'Unknown'\n",
        "    class_name = get_class_name(wnid)\n",
        "\n",
        "    # Plot image\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(f'{image_files[i]}\\nClass: {class_name}')\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    for obj in root.findall('object'):\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text)\n",
        "        ymin = int(bbox.find('ymin').text)\n",
        "        xmax = int(bbox.find('xmax').text)\n",
        "        ymax = int(bbox.find('ymax').text)\n",
        "\n",
        "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        axes[i].add_patch(rect)\n",
        "\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3w-JYuMJ2t0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Paramètres**"
      ],
      "metadata": {
        "id": "Lu1aKiuusxJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 1000\n",
        "BATCH_SIZE=32 #@param [1,2,4,8,16,32,64,128] {type:\"raw\"}\n",
        "EPOCHS=5 #@param [1,5, 10,20,50,100,200] {type:\"raw\"}\n",
        "Learning_rate = 0.001 #@param [0.1, 0.01,0.02,0.05,0.001,0.002,0.005] {type:\"raw\"}\n",
        "Train_split = 0.8  # @param [0.7, 0.8, 0.9] {type:\"raw\"}\n",
        "# Percentage of remaining data allocated to test\n",
        "Test_ratio = 0.5  # @param [0.3, 0.4, 0.5, 0.6, 0.7] {type:\"raw\"}\n",
        "Val_split = (1 - Train_split) * (1 - Test_ratio)\n",
        "Test_split = 1 - Train_split - Val_split\n",
        "Img_size = 224 #@param [224,299] {type:\"raw\"}\n",
        "Accelerator= \"auto\" #@param [\"cpu\",\"gpu\",\"auto\"]\n",
        "num_workers = 4 #@param [1,2,4,8,16] {type:\"raw\"}\n",
        "# Weight for localization loss\n",
        "lambda_loc = 0.1  #@param [0.1, 0.2, 0.5, 0.9, 1] {type:\"raw\"}\n",
        "# size of the dataset (we don't take the full dataset for faster training)\n",
        "subsample_size = 1000 #@param [100,1000,5000,10000,50000] {type:\"raw\"}\n",
        "DATA_DIR=\".\"\n",
        "LOG_DIR=\"logs/\""
      ],
      "metadata": {
        "id": "YgbtB4aAsxSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Création de la classe pour le modèle**"
      ],
      "metadata": {
        "id": "KXalpgu2vds4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNSimpleModel(pl.LightningModule):\n",
        "    def __init__(self, model, num_classes=NUM_CLASSES, lambda_loc=1.0):\n",
        "        super().__init__()\n",
        "        # self.model = model\n",
        "        # Store original model reference\n",
        "        self.original_model = model\n",
        "\n",
        "        # Modify classifier in-place\n",
        "        in_features = self.original_model.model.classifier[1].in_features\n",
        "        self.original_model.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "        # Register modified model as submodule\n",
        "        self.model = self.original_model.model\n",
        "\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.lambda_loc = lambda_loc\n",
        "        # Initialize confusion matrix metric\n",
        "        self.confusion_matrix = MulticlassConfusionMatrix(num_classes=self.num_classes)\n",
        "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
        "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
        "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return self.model(x)\n",
        "        logits, features = self.model(x)\n",
        "        return logits, features\n",
        "\n",
        "    def bbox_to_mask(self, bboxes, height=Img_size, width=Img_size):\n",
        "        \"\"\"\n",
        "        Convert bounding box coordinates to a binary mask.\n",
        "        Args:\n",
        "            bboxes: Tensor of shape [batch_size, 4] containing bounding box coordinates.\n",
        "            height: Height of the mask.\n",
        "            width: Width of the mask.\n",
        "        Returns:\n",
        "            mask: Tensor of shape [batch_size, height, width] containing binary masks.\n",
        "        \"\"\"\n",
        "        batch_size = bboxes.size(0)\n",
        "        mask = torch.zeros(batch_size, height, width, device=bboxes.device)\n",
        "        for i in range(batch_size):\n",
        "            xmin, ymin, xmax, ymax = bboxes[i]\n",
        "            # Convert bounding box coordinates to integers\n",
        "            xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
        "            mask[i, ymin:ymax, xmin:xmax] = 1.0\n",
        "        return mask\n",
        "\n",
        "    def localization_loss(self, features, gt_bboxes):\n",
        "        \"\"\"\n",
        "        Compute localization loss using feature maps and bounding box masks.\n",
        "        Args:\n",
        "            features: Tensor of shape [batch_size, channels, height, width].\n",
        "            gt_bboxes: Tensor of shape [batch_size, 4] containing bounding box coordinates.\n",
        "        Returns:\n",
        "            loc_loss: Localization loss.\n",
        "        \"\"\"\n",
        "        # Convert bounding boxes to binary masks\n",
        "        gt_masks = self.bbox_to_mask(gt_bboxes)\n",
        "        # Resize features to match mask dimensions\n",
        "        features_resized = F.interpolate(features, size=(Img_size, Img_size), mode='bilinear', align_corners=False)\n",
        "        # Compute localization loss\n",
        "        loc_loss = F.l1_loss(features_resized.mean(dim=1), gt_masks, reduction='mean')\n",
        "        return loc_loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels, bboxes = batch\n",
        "        logits, features = self(images)\n",
        "        loss_cls = F.cross_entropy(logits, labels)\n",
        "        loss_loc = self.localization_loss(features, bboxes)\n",
        "        loss_total = loss_cls + self.lambda_loc * loss_loc\n",
        "        acc = self.train_accuracy(logits.argmax(dim=1), labels)\n",
        "\n",
        "        # Log training loss\n",
        "        self.log_dict({'train_loss_cls':loss_cls,'train_loss_loc':loss_loc,'train_loss':loss_total,\"train_acc\":acc}, on_step=True,prog_bar=True,logger=True, on_epoch=True)\n",
        "        return loss_total\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.train_accuracy.reset()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels, bboxes = batch\n",
        "        logits, features = self(images)\n",
        "        loss_cls = F.cross_entropy(logits, labels)\n",
        "        loss_loc = self.localization_loss(features, bboxes)\n",
        "        loss_total = loss_cls + self.lambda_loc * loss_loc\n",
        "        acc = self.val_accuracy(logits.argmax(dim=1), labels)\n",
        "\n",
        "        # Log val loss\n",
        "        self.log_dict({'val_loss_cls':loss_cls,'val_loss_loc':loss_loc,'val_loss':loss_total,\"val_acc\":acc}, on_step=True,prog_bar=True,logger=True, on_epoch=True)\n",
        "        return loss_total\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.val_accuracy.reset()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, labels, bboxes = batch\n",
        "        print(f\"Processing batch {batch_idx+1}\")\n",
        "        logits, features = self(images)\n",
        "        loss_cls = F.cross_entropy(logits, labels)\n",
        "        loss_loc = self.localization_loss(features, bboxes)\n",
        "        loss_total = loss_cls + self.lambda_loc * loss_loc\n",
        "        acc = self.test_accuracy(logits.argmax(dim=1), labels)\n",
        "\n",
        "        print(f\"Predictions: {logits.argmax(dim=1)[:5]}\")  # Print first 5 predictions\n",
        "\n",
        "        # Log test loss\n",
        "        self.log_dict({'test_loss_cls':loss_cls,'test_loss_loc':loss_loc,'test_loss':loss_total,\"test_acc\":acc}, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.confusion_matrix.update(logits.argmax(dim=1), labels)\n",
        "\n",
        "        #return {\"loss\": loss_total, \"acc\": acc}\n",
        "        return loss_total\n",
        "\n",
        "    def on_test_end(self):\n",
        "        print(\"Test finished!\")\n",
        "        self.test_accuracy.reset()\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        print(\"Generating confusion matrix...\")\n",
        "        cm = self.confusion_matrix.compute().cpu()\n",
        "\n",
        "        # Sum rows and columns to find most frequent classes\n",
        "        row_sums = cm.sum(dim=1)\n",
        "        top_n_indices = row_sums.argsort(descending=True)[:20]  # Get top 20 classes\n",
        "\n",
        "        # Extract subset of confusion matrix\n",
        "        cm_subset = cm[top_n_indices][:, top_n_indices]\n",
        "\n",
        "        # Plot subset\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        sns.heatmap(cm_subset.numpy(), annot=True, fmt=\"d\", ax=ax)\n",
        "        ax.set_title(\"Confusion Matrix (Top 20 Classes)\")\n",
        "        plt.show()\n",
        "\n",
        "        self.confusion_matrix.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # return torch.optim.SGD(self.parameters(), lr=0.01)\n",
        "        optimizer = optim.Adam(self.parameters(), lr=Learning_rate)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "weqXS9rUH1l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Création du premier modèle**"
      ],
      "metadata": {
        "id": "YJrg_p2BwACS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model MobileNet\n",
        "class MobilenetModel(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "        #self.model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "        # Freeze all layers except the last convolutional layer\n",
        "        for param in self.model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.model.features[-1].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Replace the classifier head\n",
        "        self.model.classifier[1] = nn.Linear(self.model.last_channel, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract feature maps from the last convolutional layer\n",
        "        features = self.model.features(x)\n",
        "        # Global average pooling\n",
        "        pooled_features = F.adaptive_avg_pool2d(features, (1, 1)).squeeze(-1).squeeze(-1)\n",
        "        # Classifier\n",
        "        logits = self.model.classifier(pooled_features)\n",
        "        return logits, features"
      ],
      "metadata": {
        "id": "EPW9iMIBwAIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mn1 = MobilenetModel()\n",
        "mn1.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "summary(mn1,(3,Img_size,Img_size))"
      ],
      "metadata": {
        "id": "pD-gbijvwJJZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oNHetqvJxCuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Création des jeux de données d'entraînement, validation et test \"Data Loaders\"** #"
      ],
      "metadata": {
        "id": "nIkP5sr2WKeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class ImageNetDataset(Dataset):\n",
        "    def __init__(self, image_folder, annotation_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.annotation_folder = annotation_folder\n",
        "        self.image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.JPEG')])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_files[idx]\n",
        "        image_path = os.path.join(self.image_folder, image_name)\n",
        "        annotation_path = os.path.join(self.annotation_folder, f'{os.path.splitext(image_name)[0]}.xml')\n",
        "\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "        class_idx = -1\n",
        "        bbox = None\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            wnid = root.find('object/name').text if root.find('object/name') is not None else 'Unknown'\n",
        "\n",
        "            # Convert class name to class index\n",
        "            for key, value in class_mapping.items():\n",
        "                if value[1].replace('_', ' ') == get_class_name(wnid):\n",
        "                    class_idx = int(key)\n",
        "                    break\n",
        "\n",
        "            obj = root.find('object/bndbox')\n",
        "            if obj is not None:\n",
        "                bbox = [int(obj.find(tag).text) for tag in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "                # Scale bounding box to resized image dimensions\n",
        "                bbox = [\n",
        "                    bbox[0] * 224 // img.width,\n",
        "                    bbox[1] * 224 // img.height,\n",
        "                    bbox[2] * 224 // img.width,\n",
        "                    bbox[3] * 224 // img.height\n",
        "                ]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, torch.tensor(class_idx, dtype=torch.long), torch.tensor(bbox) if bbox else torch.zeros(4)\n",
        "\n",
        "# Wrapper to apply different transforms to subsets\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, class_idx, bbox = self.subset[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, class_idx, bbox\n",
        "\n",
        "def create_data_loaders(image_path, bb_path, batch_size=BATCH_SIZE,\n",
        "                         img_size=Img_size, num_workers=num_workers):\n",
        "    # Define transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create main dataset\n",
        "    main_dataset = ImageNetDataset(image_path, bb_path, transform=None)\n",
        "\n",
        "    # Subsample main dataset\n",
        "    subsample_indices = random.sample(range(len(main_dataset)), subsample_size)\n",
        "    dataset = Subset(main_dataset, subsample_indices)\n",
        "\n",
        "    # Split indices\n",
        "    dataset_size = len(dataset)\n",
        "    indices = np.arange(dataset_size)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_end = int(Train_split * dataset_size)\n",
        "    val_end = train_end + int(Val_split * dataset_size)\n",
        "\n",
        "    train_indices, val_indices, test_indices = indices[:train_end], indices[train_end:val_end], indices[val_end:]\n",
        "\n",
        "    # Create subsets with transforms\n",
        "    train_subset = TransformDataset(Subset(dataset, train_indices), transform=train_transform)\n",
        "    val_subset = TransformDataset(Subset(dataset, val_indices), transform=train_transform)\n",
        "    test_subset = TransformDataset(Subset(dataset, test_indices), transform=test_transform)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "Akc2TK6IWK0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Définir les hyper-paramètres, EarlyStopping, Checkpoints** #"
      ],
      "metadata": {
        "id": "wGYAnQ4bjS5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_loader, val_loader, test_loader = create_data_loaders(extract_path, bbox_annotation_path, BATCH_SIZE, Img_size, num_workers)\n",
        "\n",
        "# Initialize model\n",
        "model = CNNSimpleModel(mn1)\n",
        "\n",
        "# Setup callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    dirpath=models_folder, #Save the best model in Google Drive\n",
        "    filename='best-checkpoint-mobilenet', #filename='best-checkpoint-mobilenet-{epoch:02d}-{val_loss:.2f}'\n",
        "    save_top_k=1,\n",
        "    mode='min'\n",
        ")\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Initialize WandbLogger\n",
        "wandb_logger = WandbLogger(project=\"ResponsibleTraining\")\n",
        "\n",
        "# Initialize WandbLogger\n",
        "csv_logger = CSVLogger(LOG_DIR, name=\"cnn\", version='')\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=EPOCHS,\n",
        "    accelerator=Accelerator,\n",
        "    log_every_n_steps=1,\n",
        "    devices=1,\n",
        "    logger=[wandb_logger, csv_logger],\n",
        "    callbacks=[checkpoint_callback, early_stop_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "rbGsdcEljTFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Lancement de l'entraintement**"
      ],
      "metadata": {
        "id": "CplO1xBwwK5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "2BRTwqAtwLB6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Evaluer le modèle** ##"
      ],
      "metadata": {
        "id": "x3etU08mnaKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.test(model, dataloaders=test_loader)\n",
        "best_model_path = os.path.join(models_folder, \"best-checkpoint-mobilenet.ckpt\")\n",
        "trainer.test(dataloaders=test_loader,ckpt_path=best_model_path)"
      ],
      "metadata": {
        "id": "4-4shGNunaRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11 Exporter le modèle en .jit**"
      ],
      "metadata": {
        "id": "k9UNzYdtnaZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = CNNSimpleModel.load_from_checkpoint(best_model_path, model=MobilenetModel())\n",
        "best_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "jit_model = best_model.to_torchscript()\n",
        "jit_save_path = os.path.join(models_folder, \"best-checkpoint-mobilenet_jit.pth\")\n",
        "\n",
        "torch.jit.save(jit_model, jit_save_path)\n"
      ],
      "metadata": {
        "id": "kWhYOE0VnajC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. Afficher les courbes d'entrainement avec la fonction \"plot_metrics\"** ##"
      ],
      "metadata": {
        "id": "AjonFSDxwU0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(log_folder):\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # Load the CSV file generated by CSVLogger\n",
        "  df = pd.read_csv(f'{LOG_DIR}/{log_folder}/metrics.csv')\n",
        "  train_df = df[df['train_loss_epoch'].notna()]\n",
        "  val_df = df[df['val_loss_epoch'].notna()]\n",
        "\n",
        "  # Plot training loss\n",
        "  plt.plot(train_df['epoch'], train_df['train_loss_epoch'], label='Train Loss')\n",
        "  plt.plot(val_df['epoch'], val_df['val_loss_epoch'], label='Validation Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training & Validation Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training accuracy\n",
        "  plt.plot(train_df['epoch'], train_df['train_acc_epoch'], label='Train Acc')\n",
        "  plt.plot(val_df['epoch'], val_df['val_acc_epoch'], label='Val Acc')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Training & Validation Accuracy')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xODS3lVgwYvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(\"cnn\")"
      ],
      "metadata": {
        "id": "I_p5EIuBwoag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. Evaluation du modèle selon différentes métriques de notre librarie \"Benchmark\"** #"
      ],
      "metadata": {
        "id": "0tz1t1bpfbnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_input = torch.randn(1, 3, Img_size, Img_size)\n",
        "results = benchmark(best_model, example_input)\n",
        "\n",
        "# log to wandb\n",
        "wandb.log({\n",
        "    \"benchmark\": results\n",
        "})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "NqFHauwNfbzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **13. Tester le modèle avec une image de test de votre choix**"
      ],
      "metadata": {
        "id": "x2n1BLrsfzv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import LayerGradCam\n",
        "from pytorch_lightning import LightningModule\n",
        "import cv2\n",
        "\n",
        "def get_last_conv_layer(model: LightningModule):\n",
        "    \"\"\"Find the last convolutional layer in the model\"\"\"\n",
        "    last_conv = None\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            last_conv = module\n",
        "    if last_conv is None:\n",
        "        raise ValueError(\"No convolutional layer found in model\")\n",
        "    return last_conv\n",
        "\n",
        "def generate_gradcam(model, input_tensor, target_class, last_conv_layer):\n",
        "    \"\"\"Generate Grad-CAM heatmap using Captum\"\"\"\n",
        "    gradcam = LayerGradCam(model, last_conv_layer)\n",
        "    attribution = gradcam.attribute(input_tensor, target=target_class, relu_attributions=True)\n",
        "    return attribution[0].cpu().detach().numpy()\n",
        "\n",
        "def generate_cam(features, weights, class_idx):\n",
        "    \"\"\"Generate Class Activation Map (CAM)\"\"\"\n",
        "    class_weights = weights[class_idx]\n",
        "    cam = torch.matmul(class_weights, features.view(features.size(1), -1))\n",
        "    cam = cam.view(features.size(2), features.size(3))\n",
        "    cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
        "    return cam.cpu().numpy()\n",
        "\n",
        "def visualize_results(image, bbox, cam, gradcam, predicted_class, true_class, class_mapping):\n",
        "    \"\"\"Visualize results with 3 subplots\"\"\"\n",
        "    # Denormalize image\n",
        "    image = image.permute(1, 2, 0).cpu().numpy()\n",
        "    image = (image * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "    image = np.clip(image, 0, 1)\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "\n",
        "    # Original image with bbox\n",
        "    axes[0].imshow(image)\n",
        "    if bbox.sum() > 0:\n",
        "        xmin, ymin, xmax, ymax = bbox\n",
        "        rect = plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin,\n",
        "                           linewidth=2, edgecolor='r', facecolor='none')\n",
        "        axes[0].add_patch(rect)\n",
        "    axes[0].set_title(f\"True: {class_mapping[str(true_class)][1]}\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # CAM overlay\n",
        "    axes[1].imshow(image)\n",
        "    axes[1].imshow(cam, cmap='jet', alpha=0.5)\n",
        "    axes[1].set_title(f\"Predicted: {class_mapping[str(predicted_class)][1]}\\nCAM\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Grad-CAM overlay\n",
        "    axes[2].imshow(image)\n",
        "    axes[2].imshow(gradcam, cmap='jet', alpha=0.5)\n",
        "    axes[2].set_title(f\"Predicted: {class_mapping[str(predicted_class)][1]}\\nGrad-CAM\")\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get random sample from dataset\n",
        "dataset = ImageNetDataset(extract_path, bbox_annotation_path, transform=None)\n",
        "random_idx = random.randint(0, len(dataset)-1)\n",
        "image, true_class, bbox = dataset[random_idx]\n",
        "\n",
        "# Preprocess image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((Img_size, Img_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "input_tensor = transform(image).unsqueeze(0).to(best_model.device)\n",
        "\n",
        "# Get predictions and features\n",
        "best_model.eval()\n",
        "with torch.no_grad():\n",
        "    logits, features = best_model(input_tensor)\n",
        "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "    predicted_class = torch.argmax(probs).item()\n",
        "\n",
        "# Generate CAM\n",
        "weights = best_model.model.classifier[1].weight.data\n",
        "cam = generate_cam(features, weights, predicted_class)\n",
        "\n",
        "# Generate Grad-CAM\n",
        "last_conv_layer = get_last_conv_layer(best_model.model)\n",
        "gradcam = generate_gradcam(best_model.model, input_tensor, predicted_class, last_conv_layer)\n",
        "\n",
        "# Resize Grad-CAM to match image size\n",
        "gradcam = cv2.resize(gradcam[0], (Img_size, Img_size))\n",
        "gradcam = np.maximum(gradcam, 0)\n",
        "gradcam = gradcam / gradcam.max()\n",
        "\n",
        "# Visualize results\n",
        "visualize_results(image, bbox, cam, gradcam, predicted_class, true_class, class_mapping)\n",
        "\n",
        "# Print probabilities\n",
        "print(\"Prediction probabilities:\")\n",
        "for idx, prob in enumerate(probs[0]):\n",
        "    print(f\"{class_mapping[str(idx)][1]:<25} {prob.item()*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "NETAyCTdfz5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}