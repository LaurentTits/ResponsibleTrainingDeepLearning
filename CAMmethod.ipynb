{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMx6UdcfFNbY6i4vMK90Mr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaurentTits/ResponsibleTrainingDeepLearning/blob/main/CAMmethod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YHsAbdG2MR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 100  # mini-ImageNet has 100 classes\n",
        "LR = 1e-4\n",
        "EPOCHS = 10\n",
        "ALPHA = 1.0  # Weight for CAM loss\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- DATASET PREPARATION ---\n",
        "class MiniImageNetDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.bbox_masks = []\n",
        "\n",
        "        # Load dataset\n",
        "        for label in os.listdir(root):\n",
        "            label_dir = os.path.join(root, label)\n",
        "            if not os.path.isdir(label_dir):\n",
        "                continue\n",
        "            class_idx = int(label)  # Assuming folder names are numeric class indices\n",
        "            for img_file in os.listdir(label_dir):\n",
        "                img_path = os.path.join(label_dir, img_file)\n",
        "                bbox_path = img_path.replace(\".jpg\", \"_bbox.npy\")  # Assume bbox mask exists\n",
        "\n",
        "                if os.path.exists(bbox_path):\n",
        "                    self.data.append(img_path)\n",
        "                    self.labels.append(class_idx)\n",
        "                    self.bbox_masks.append(bbox_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.data[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "        bbox = np.load(self.bbox_masks[idx])  # Load bounding box mask (grayscale)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, (label, torch.tensor(bbox, dtype=torch.float32))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = MiniImageNetDataset(root=\"path_to_mini_imagenet/train\", transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# --- VGG16 MODEL WITH CAM ---\n",
        "class VGGWithCAM(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.vgg = models.vgg16(pretrained=True)\n",
        "        self.features = self.vgg.features\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        pooled = F.adaptive_avg_pool2d(features, (1, 1))\n",
        "        pooled = torch.flatten(pooled, 1)\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits, features\n",
        "\n",
        "# --- COMPUTE CAM ---\n",
        "def compute_cam(feature_maps, class_idx):\n",
        "    weights = model.classifier.weight[class_idx]\n",
        "    cam = torch.einsum(\"chw,c->hw\", feature_maps[0], weights)\n",
        "    cam = F.relu(cam)\n",
        "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6)\n",
        "    return cam\n",
        "\n",
        "# --- PREPROCESS BOUNDING BOX ---\n",
        "def preprocess_bbox(mask, size=(7, 7), blur_kernel=5):\n",
        "    mask = cv2.resize(mask, size, interpolation=cv2.INTER_LINEAR)\n",
        "    mask = cv2.GaussianBlur(mask, (blur_kernel, blur_kernel), 0)\n",
        "    mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-6)\n",
        "    return torch.tensor(mask, dtype=torch.float32)\n",
        "\n",
        "# --- CUSTOM LOSS FUNCTION ---\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self, alpha=ALPHA):\n",
        "        super().__init__()\n",
        "        self.classification_loss = nn.CrossEntropyLoss()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, logits, features, targets, gt_bbox_masks):\n",
        "        cls_loss = self.classification_loss(logits, targets)\n",
        "        mse_loss = 0.0\n",
        "\n",
        "        for i in range(len(targets)):\n",
        "            cam = compute_cam(features[i].unsqueeze(0), targets[i])\n",
        "            blurred_bbox = preprocess_bbox(gt_bbox_masks[i])\n",
        "            mse_loss += F.mse_loss(cam, blurred_bbox)\n",
        "\n",
        "        mse_loss /= len(targets)\n",
        "        return cls_loss + self.alpha * mse_loss\n",
        "\n",
        "# --- TRAINING SETUP ---\n",
        "model = VGGWithCAM(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = CustomLoss(alpha=ALPHA)\n",
        "\n",
        "# --- TRAINING LOOP ---\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, (labels, gt_masks) in train_loader:\n",
        "        images, labels, gt_masks = images.to(DEVICE), labels.to(DEVICE), gt_masks.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, features = model(images)\n",
        "        loss = criterion(logits, features, labels, gt_masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "UpRqSQGz4lSD",
        "outputId": "bf9b7fb7-ccb7-40aa-dfcb-86d9cc60c5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_mini_imagenet/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0f61205acc56>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m ])\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniImageNetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"path_to_mini_imagenet/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0f61205acc56>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mlabel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_mini_imagenet/train'"
          ]
        }
      ]
    }
  ]
}